{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Finetune ResNet\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.13.1\n",
      "Torchvision Version:  0.14.1\n",
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "from utils_webots import *\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"squeezenet\"\n",
    "num_classes = 1\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "from cnn import ConvNeuralNet\n",
    "model_ft = ConvNeuralNet(1)  # one class\n",
    "\n",
    "# model_ft = models.squeezenet1_0(weights= True)\n",
    "# model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "# model_ft.num_classes = num_classes\n",
    "\n",
    "# model_ft = models.resnet18(weights=True)\n",
    "## set_parameter_requires_grad(model_ft, feature_extract)\n",
    "# num_ftrs = model_ft.fc.in_features\n",
    "# model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "# input_size = 224\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "data_transforms = transforms.Compose([transforms.ToTensor()]) #, transforms.Resize((input_size, input_size)), ])\n",
    "dataset = WeBotsDataset(filename='df_follower_new', transform=data_transforms) # WeBotsDataset(transform=data_transforms)\n",
    "\n",
    "# # Create training and validation datasets\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "dataloaders_dict = {'train': train_loader, 'val': test_loader}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Sanity check\n",
    "img, target = next(iter(dataset))\n",
    "# plt.imshow(img[0]), target\n",
    "model_ft(img.cuda().float().unsqueeze(0))\n",
    "\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "params_to_update = model_ft.parameters()\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train and evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T19:37:00.364760676Z",
     "start_time": "2023-05-06T19:36:55.847518172Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([7.000e+00, 3.000e+00, 3.000e+00, 3.000e+00, 3.000e+00, 3.000e+00,\n        3.000e+00, 6.000e+00, 4.000e+00, 4.000e+00, 3.000e+00, 1.100e+01,\n        1.000e+01, 2.500e+01, 8.700e+01, 3.010e+02, 6.880e+02, 5.790e+02,\n        9.860e+02, 1.526e+03, 1.722e+03, 1.669e+03, 2.635e+03, 1.645e+03,\n        1.052e+03, 6.100e+02, 4.100e+02, 5.720e+02, 4.450e+02, 4.490e+02,\n        2.340e+02, 1.950e+02, 1.300e+02, 1.480e+02, 5.900e+01, 3.760e+02,\n        5.680e+02, 5.540e+02, 4.140e+02, 6.000e+01, 1.400e+01, 1.100e+01,\n        8.000e+00, 2.000e+00, 3.000e+00, 3.000e+00, 0.000e+00, 0.000e+00,\n        0.000e+00, 5.000e+00]),\n array([-0.5     , -0.482968, -0.465936, -0.448904, -0.431872, -0.41484 ,\n        -0.397808, -0.380776, -0.363744, -0.346712, -0.32968 , -0.312648,\n        -0.295616, -0.278584, -0.261552, -0.24452 , -0.227488, -0.210456,\n        -0.193424, -0.176392, -0.15936 , -0.142328, -0.125296, -0.108264,\n        -0.091232, -0.0742  , -0.057168, -0.040136, -0.023104, -0.006072,\n         0.01096 ,  0.027992,  0.045024,  0.062056,  0.079088,  0.09612 ,\n         0.113152,  0.130184,  0.147216,  0.164248,  0.18128 ,  0.198312,\n         0.215344,  0.232376,  0.249408,  0.26644 ,  0.283472,  0.300504,\n         0.317536,  0.334568,  0.3516  ]),\n <BarContainer object of 50 artists>)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dataset.df_loaded['angle'],bins=50)\n",
    "\n",
    "# model_ft\n",
    "# i, t = next(iter(train_loader))\n",
    "# model_ft(img[0].unsqueeze(0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T19:54:26.580092889Z",
     "start_time": "2023-05-06T19:54:26.493324305Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.0198, -0.0025,  0.0008,  0.0016, -0.0301, -0.0140, -0.0092,  0.0056,\n        -0.0213, -0.0022,  0.0083,  0.0045, -0.0130, -0.0045, -0.0044, -0.0045],\n       device='cuda:0', grad_fn=<SqueezeBackward0>)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# img = next(iter(train_loader))\n",
    "# plt.imshow(img[0][0][0])\n",
    "# img[0].shape\n",
    "# model_ft(img[0].cuda())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T19:22:11.094776687Z",
     "start_time": "2023-05-06T19:22:10.916185525Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 0.5513\n",
      "val Loss: 0.0021 Acc: 0.6668\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteogu/miniconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0021 Acc: 0.5531\n",
      "val Loss: 0.0019 Acc: 0.6854\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.5594\n",
      "val Loss: 0.0018 Acc: 0.6873\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.5607\n",
      "val Loss: 0.0018 Acc: 0.6881\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.5632\n",
      "val Loss: 0.0017 Acc: 0.6884\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.5657\n",
      "val Loss: 0.0017 Acc: 0.6816\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.5674\n",
      "val Loss: 0.0016 Acc: 0.6783\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.5698\n",
      "val Loss: 0.0016 Acc: 0.6742\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.5705\n",
      "val Loss: 0.0016 Acc: 0.6679\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.5726\n",
      "val Loss: 0.0016 Acc: 0.6643\n",
      "\n",
      "Training complete in 0m 38s\n",
      "Best val Acc: 0.688408\n"
     ]
    }
   ],
   "source": [
    "model, dataloaders, optimizer, num_epochs, is_inception = model_ft, dataloaders_dict, optimizer_ft, num_epochs, (model_name==\"inception\")\n",
    "\n",
    "since = time.time()\n",
    "\n",
    "val_acc_history = []\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()  # Set model to training mode\n",
    "        else:\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, (_, labels) in dataloaders[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                # Get model outputs and calculate loss\n",
    "                # Special case for inception because in training it has an auxiliary output. In train\n",
    "                #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                #   but in testing we only consider the final output.\n",
    "                outputs = model(inputs)\n",
    "                # print(outputs.shape, labels.shape)\n",
    "                # loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "                # print(outputs.shape, labels.shape)\n",
    "                loss = criterion(outputs, labels.float())\n",
    "\n",
    "                # print(loss)\n",
    "                # _, preds = torch.max(outputs, 1)\n",
    "                preds = outputs\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum( (preds - labels.data) < 1e-2) # just a threshold\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "        # deep copy the model\n",
    "        if phase == 'val' and epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        if phase == 'val':\n",
    "            val_acc_history.append(epoch_acc)\n",
    "\n",
    "    print()\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "# load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "torch.save(model.cpu().state_dict(), 'model_weights_small_angle.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T19:39:40.975696104Z",
     "start_time": "2023-05-06T19:39:03.365391460Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(-0.1711, device='cuda:0'), tensor([-0.1953], device='cuda:0'))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, labels.data\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T19:39:47.226399201Z",
     "start_time": "2023-05-06T19:39:47.218620094Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_weights.pth\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:798\u001B[0m, in \u001B[0;36mModule.cpu\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcpu\u001B[39m(\u001B[38;5;28mself\u001B[39m: T) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    790\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the CPU.\u001B[39;00m\n\u001B[1;32m    791\u001B[0m \n\u001B[1;32m    792\u001B[0m \u001B[38;5;124;03m    .. note::\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    796\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    797\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 798\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 641\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    644\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    645\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    646\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    651\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    652\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:664\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    660\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    661\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    662\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    663\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 664\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    665\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    666\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:798\u001B[0m, in \u001B[0;36mModule.cpu.<locals>.<lambda>\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m    789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcpu\u001B[39m(\u001B[38;5;28mself\u001B[39m: T) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    790\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the CPU.\u001B[39;00m\n\u001B[1;32m    791\u001B[0m \n\u001B[1;32m    792\u001B[0m \u001B[38;5;124;03m    .. note::\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    796\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    797\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 798\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply(\u001B[38;5;28;01mlambda\u001B[39;00m t: \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "torch.save(model.cpu().state_dict(), 'model_weights.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T07:14:03.465879716Z",
     "start_time": "2023-05-06T07:14:03.401028182Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
